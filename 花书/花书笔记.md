# Deep Learning 深度学习

Ian Goodfellow, Yoshua Bengio, Aaron Courville

## Part I Applied Math and Machine Learning Basics

### Chapter 2 Linear Algebra
#### 2.1 Scalars, Vectors, Matrices and Tensors
* **主对角线(main diagonal)**: 从左上角到右下角的对角线
* **broadcasting**:
$$C = A + b$$
其中$$C_{i, j} = A_{i, j} + b_j$$

#### 2.2 Multiplying Matrices and Vectors
* **矩阵乘积**: $C = AB$
* **Hadamard乘积(Hadamard-Product)**: 元素对应乘积
* 两个向量的点积满足交换律:
$$x^Ty=y^Tx$$

#### 2.3 Identity and Inverse Matrices 单位矩阵和逆矩阵
**LU分解**:矩阵求逆的高效算法

#### 2.4 Linear Dependence and Span 线性相关和生成子空间

#### 2.5 Norms 范数
* **范数(norm)**: 衡量向量的大小
* **$L^p范数$**:
$$
||x_p||=(\sum_{i}|x_i|^p)^\frac{1}{p}
$$

* $L^2$范数对每个元素求导和整个向量相关
* 平方$L^2$范数对$x$求导只取决于对应的元素， 但是其求导在原点处增长十分缓慢
* $L^1$范数在各个位置斜率相同
* $L^1$范数经常作为表示非零元素数目的替代函数？
* **$L^{\infty}$范数**: 最大范数(max norm)
$$
||x||_{\infty} = \max_{i}|x_i|
$$

#### 2.6 Special Kinds of Matrices and Vectors

#### 2.7 EigenDecomposition 特征分解
$$
Av = \lambda v
$$
$$
A = Vdiag(\lambda)V^{-1}
$$

#### 2.8 Singular Value Decomposition (SVD)

$$
A = UDV^T
$$
